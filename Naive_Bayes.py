# -*- coding: utf-8 -*-
"""Naive Bayes Doc

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OtvI54K7fApRCFjEDoXJRKK1phNF7r-o
"""

wfrom google.colab import drive
drive.mount('/content/drive')


#Dataset: https://archive.ics.uci.edu/dataset/264/eeg+eye+state

from scipy.io import arff
import pandas as pd
import numpy as np

#Load the dataset
file_path = '/content/drive/MyDrive/Final_Project_Stats/EEG_Eye_State.arff'
data, meta = arff.loadarff(file_path)
df = pd.DataFrame(data)
data = df.to_numpy() #Shape 14930, 15

#Create array of labels and data

labels = data[:, 14]
decoded_labels = np.array([x.decode('utf-8') for x in labels]) #Shape 14930, 14
print(decoded_labels)
decoded_labels = decoded_labels.astype(int)

data = np.delete(data, 14, 1) #Shape 14930, 14
print(data)

sampling_rate = round(14930 / 117)

'''
PREPROCESSING

1) Removing extremely high/low voltage measurements
2) Rereferencing Data (Common Average Referencing)
3) Apply a Bandpass filter (1hz to 40hz) to isolate important frequencies
4) Sort the labels into epochs (for each model that we use. Each model's individual preprocessing starts here)
5) Normalize the data within each epoch
6) Compute the power spectral density for each epoch
'''

mask = np.ones((len(data)), dtype=bool)  # start with all True

for i in range(14):
    Q1 = np.percentile(data[:, i], 25)
    Q3 = np.percentile(data[:, i], 75)
    IQR = Q3 - Q1

    outlier_mask = (data[:, i] < (Q1 - 10 * IQR)) | (data[:, i] > (Q3 + 5 * IQR))
    mask = ~outlier_mask & mask

filtered_data = data[mask]
decoded_labels = decoded_labels[mask]
print(filtered_data.shape)
print(decoded_labels.shape)

#Reference data

average = filtered_data.mean(axis = 1, keepdims = True)
print(average.shape)

average_data = filtered_data - average
print(filtered_data)

from scipy.signal import butter, filtfilt

def bandpass_filter(data, lowcut, highcut, fs, order=4):
    """
    Apply a zero-phase bandpass filter to EEG data.

    Parameters:
    - data: EEG signal, shape (n_samples, n_channels)
    - lowcut: Low frequency cutoff (e.g., 1 Hz)
    - highcut: High frequency cutoff (e.g., 40 Hz)
    - fs: Sampling rate in Hz (e.g., 512)
    - order: Filter order (default = 4)

    Returns:
    - filtered_data: same shape as input
    """
    nyq = 0.5 * fs  # Nyquist frequency
    low = lowcut / nyq
    high = highcut / nyq

    b, a = butter(order, [low, high], btype='band')      # Bandpass filter coefficients
    filtered_data = filtfilt(b, a, data, axis=0)          # Zero-phase filtering (no phase shift)
    return filtered_data

bandpass_filtered_data = bandpass_filter(average_data, 1, 40, sampling_rate)

#Plot the EEG Data
import matplotlib.pyplot as plt

def plot_eeg_signal(eeg_data, channel, sampling_rate, start, end):

  plt.figure(figsize=(10, 6))
  plt.plot(eeg_data[(start * 117) : (end*117), channel], label=f'Channel {channel}')

  plt.xlabel('Time (frames)')
  plt.title('EEG Signal (first few channels)')
  plt.legend()
  plt.grid(True)
  plt.tight_layout()
  plt.show()


plot_eeg_signal(bandpass_filtered_data, 3, sampling_rate, 0, 15)

import numpy as np
import matplotlib.pyplot as plt

def plot_labels(labels, epoch_duration, title):

  epoch_duration_frames = 1/(128*epoch_duration)  # in seconds (change this if using 0.5s or 2s epochs)

  n_labels = len(labels)
  time_axis = np.arange(n_labels) * epoch_duration

  labels_2d = labels.reshape(1, -1)

  plt.figure(figsize=(12, 1.5))
  plt.imshow(labels_2d, aspect='auto', cmap='coolwarm', interpolation='nearest', extent=[0, n_labels * epoch_duration_frames, 0, 1])



  plt.yticks([])  # Hide y-axis (only one row)
  tick_interval = 5  # seconds between x-ticks
  xticks = np.arange(0, n_labels * epoch_duration_frames + 1, tick_interval)
  plt.xticks(xticks)
  plt.xlabel('Time (seconds)')
  plt.title(f'{title}')
  plt.colorbar(label='Eye State')
  plt.tight_layout()
  plt.show()

plot_labels(decoded_labels, 1, "Actual Labels Whole dataset")

'''
Start of preprocessing for Naive Bayes model
'''

epoch_labels = np.array([])
epoch_data = []


#This sorts the data in 0.5s windows
for i in range(230):
  labels_subset = decoded_labels[i*64 : (i+1)*64]
  epoch_labels = np.append(epoch_labels, round(np.mean(labels_subset)))

  data_subset = bandpass_filtered_data[i*64 : (i+1)*64, :]
  epoch_data.append(data_subset)

epoch_data = np.array(epoch_data) #Shape 230, 64, 15 : Time Bins, Frames, Channels
epoch_labels = epoch_labels.astype(int) #Shape 230 : Time Bins

from scipy.signal import welch
from scipy.stats import entropy as shannon_entropy
import numpy as np

def compute_bandpower(data, fs, band):
    """Compute average power in a frequency band using Welchâ€™s method."""
    fmin, fmax = band
    freqs, psd = welch(data, fs=fs, nperseg=min(64, len(data)))
    freq_res = freqs[1] - freqs[0]
    idx_band = np.logical_and(freqs >= fmin, freqs <= fmax)
    return np.sum(psd[idx_band]) * freq_res

# Define EEG bands
bands = {
    'delta': (1, 4),
    'theta': (4, 8),
    'alpha': (8, 12),
    'beta': (13, 30),
}

fs = 128  # Adjust if needed
summary_features = []

for bin_data in epoch_data:  # shape (64, 15)
    features = []

    for ch in range(bin_data.shape[1]):  # for each channel
        signal = bin_data[:, ch]  # shape (64,)

        # Time-domain features
        mean = np.mean(signal)
        std = np.std(signal)

        # Frequency-domain features (Welch bandpower)
        bandpowers = [compute_bandpower(signal, fs, band) for band in bands.values()]

        # Entropy
        hist, _ = np.histogram(signal, bins=32, density=True)
        ent = shannon_entropy(hist + 1e-8)  # avoid log(0)

        # Combine features
        features.extend([mean, std] + bandpowers + [ent])

    summary_features.append(features)

summary_features = np.array(summary_features)

import matplotlib.pyplot as plt

def get_feature_name(feature_position):
    feature_map = {
        0: "Mean",
        1: "Standard Deviation",
        2: "Delta Power",
        3: "Theta Power",
        4: "Alpha Power",
        5: "Beta Power",
        6: "Entropy"
    }
    return feature_map.get(feature_position, "Unknown Feature")

def plot_feature(summary_features, channel_index, feature_index):
    """
    Plot a selected feature over time for a given channel.

    Args:
        summary_features (np.ndarray): shape (time_bins, channels * features_per_channel)
        channel_index (int): index of the EEG channel
        feature_position (int): which feature to extract:
            0 = mean, 1 = std, 2 = delta, 3 = theta, 4 = alpha, 5 = beta, 6 = entropy
    """

    feature_name = get_feature_name(feature_index)

    features_per_channel = 7  # mean, std, 4 bandpowers, entropy
    time_bins = summary_features.shape[0]

    index = channel_index * features_per_channel + feature_index
    y = summary_features[:, index]

    plt.figure(figsize=(10, 4))
    plt.plot(range(time_bins), y)
    plt.title(f"(Channel {channel_index})")
    plt.xlabel("Time bin")
    plt.ylabel(feature_name)
    plt.grid(True)
    plt.tight_layout()
    plt.show()

#Shape 230, (Channels * features)
'''
Features:
mean
standard deviation
delta power
theta power
alpha power
beta power
entropy
'''


summary_features = np.abs(summary_features)

#Split the data into test / train, standardize the data


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


'''
The Y values are the labels, and the X values are the corresponding time bins
The X values are the power of different frequencies overtime (for 5 time points)
'''
X_train = summary_features[46:, :]
X_test = summary_features[:46, :]
y_train = epoch_labels[46:]
y_test = epoch_labels[:46]


#Remove complex parts of data, convert to float 32
X_train = np.abs(X_train).astype(np.float32)
X_test = np.abs(X_test).astype(np.float32)
y_train = np.abs(y_train)
y_test = np.abs(y_test)



scaler = StandardScaler()
X_train_standardized = scaler.fit_transform(X_train)  # Fit on train
X_test_standardized  = scaler.transform(X_test)        # Use same stats on test

plot_feature(X_train_standardized, 0, 4)
plot_labels(y_train, 0.5, "Training Labels")

import numpy as np

def estimate_parameters(X, Y, n_states=2):
    D = X.shape[1]  # number of features
    pi = np.zeros(n_states)
    A = np.zeros((n_states, n_states))
    mu = np.zeros((n_states, D))
    sigma = np.zeros((n_states, D, D))

    # Initial state
    if Y[0] == 0:
        pi[0] += 0.9
        pi[1] += 0.1
    else:
        pi[0] += 0.1
        pi[1] += 0.9
    pi = pi / pi.sum()


    #Transition matrix
    for t in range(1, Y.shape[0]):
      A[Y[t-1], Y[t]] += 1
    total = np.sum(A, axis=1, keepdims=True)
    A = A / total

    # Emission parameters
    for k in range(n_states):
        X_k = X[Y == k]
        mu[k] = X_k.mean(axis=0)
        sigma[k] = np.cov(X_k.T) + 1e-6 * np.eye(D)  # regularization



    return pi, A, mu, sigma

#This step finds the most relavent channels by cohens d

D = X_train_standardized.shape[1]
X = X_train_standardized
Y = y_train

X0_data = []
X1_data = []

for i in range(X.shape[0]):
  if (Y[i] == 0):
    X0_data.append(X[i])
  else:
    X1_data.append(X[i])

X0_data = np.array(X0_data)
X1_data = np.array(X1_data)

mu0 = np.mean(X0_data, axis=0)
mu1 = np.mean(X1_data, axis=0)

std0 = np.std(X0_data, axis=0)
std1 = np.std(X1_data, axis=0)


#Find the most relavent channels by cohens d

pooled_std = np.sqrt((std0**2 + std1**2) / 2)
pooled_std[pooled_std == 0] = 1e-8
cohens_d = (mu0 - mu1) / pooled_std

#Visualize the cohens d values by feature

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 4))
plt.stem(cohens_d)
plt.axhline(0.8, color='red', linestyle='--', label='Strong effect threshold')
plt.axhline(-0.8, color='red', linestyle='--')
plt.title("Cohen's d per Feature")
plt.xlabel("Feature Index")
plt.ylabel("Effect Size (Cohen's d)")
plt.legend()
plt.grid(True)
plt.show()

#Adjust the data by which features are the most important, decode which features those are
number_to_keep = 5
import numpy as np

# Get indices of top 10 features by absolute effect size
sorted_values = np.argsort(np.abs(cohens_d))
top_indices = sorted_values[-(number_to_keep):]
# Print the top 10 features and their effect sizes
for i in top_indices:
    print(f"index {i}: d = {cohens_d[i]:.3f}")

#Decode the feature number


import numpy as np

channel_number = ( top_indices // 7)
feature_number = top_indices % 7
indices = np.vstack([channel_number, feature_number])

# Convert to flat indices (assuming 7 features per channel)
features_per_channel = 7
flat_indices = channel_number * features_per_channel + feature_number

# Sort by channel
sort_order = np.argsort(channel_number)
sorted_flat_indices = flat_indices[sort_order]

# Apply to your datasets
X_train_top = X_train_standardized[:, sorted_flat_indices]
X_test_top  = X_test_standardized[:, sorted_flat_indices]

import torch
pi, A, mu, sigma = estimate_parameters(X_train_top, y_train)

pi = torch.tensor(pi, dtype=torch.float32)
A = torch.tensor(A, dtype=torch.float32)
mu = torch.tensor(mu, dtype=torch.float32)
sigma = torch.tensor(sigma, dtype=torch.float32)


#state 0 = eye open, state 1 = eye closed
print("pi shape", pi.shape) #Shape 2
print("transition matrix shape", A.shape) #Shape 2, 2
print("mu shape", mu.shape) #Shape 2, 98
print("sigma shape", sigma.shape) #Shape 2, 98, 98

from torch.distributions.multivariate_normal import MultivariateNormal

'''
Calculates the most likley states of a test_data set given the statistics
found in the training dataset


'''

def calcualte_naive_bayes(x_data, pi, A, mu, sigma):

  eye_open_dist = MultivariateNormal(loc=mu[0], covariance_matrix=sigma[0])
  eye_closed_dist = MultivariateNormal(loc=mu[1], covariance_matrix=sigma[1])

  eye_open_emissions = eye_open_dist.log_prob(x_data)
  eye_closed_emissions = eye_closed_dist.log_prob(x_data)


  #shape 230, 2
  emission_probability = torch.stack([eye_open_emissions, eye_closed_emissions], dim=1)
  print(emission_probability.shape)

  results = torch.log(pi) + (emission_probability[0])

  results = torch.zeros_like(emission_probability[:, 0])

  for i in range(x_data.shape[0]):
    print("P(open) =", emission_probability[i, 0].item(), "\nP(closed) =", emission_probability[i, 1].item())


    if emission_probability[i, 0] > emission_probability[i, 1]:
      print("eye open")
      results[i] = 0
    else:
      results[i] = 1
      print("eye closed")
  return results

X_test_top = torch.tensor(X_test_top)
results = calcualte_naive_bayes(X_test_top, pi, A, mu, sigma)

plot_labels(epoch_labels[:46], 0.5, "True Labels")
plot_labels(results, 0.5, "Naive Bayes Predicted Labels")

accuracy = torch.mean((results == y_test).float())
print(accuracy)

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
from typing import Tuple, Union

class TimeSeriesLogisticRegression:
    """
    Logistic Regression model for time series eye state prediction.
    Handles sequences of variable length by flattening time series data.
    """

    def __init__(self, random_state=42):
        self.model = LogisticRegression(random_state=random_state, max_iter=1000)
        self.scaler = StandardScaler()
        self.is_fitted = False
        self.train_seq_length = None
        self.n_features = None

    def _prepare_data(self, X: np.ndarray) -> np.ndarray:
        """
        Prepare data for logistic regression.

        Args:
            X: Array of shape (n_samples, n_features)

        Returns:
            Array ready for logistic regression
        """
        # Convert torch tensor to numpy if needed
        if hasattr(X, 'numpy'):
            X = X.numpy()
        elif hasattr(X, 'detach'):
            X = X.detach().numpy()

        # Ensure it's a numpy array
        X = np.array(X)

        # If 3D, flatten sequences; if 2D, use as is
        if len(X.shape) == 3:
            return X.reshape(X.shape[0], -1)
        return X

    def fit(self, X_train: np.ndarray, y_train: np.ndarray):
        """
        Fit the logistic regression model.

        Args:
            X_train: Training data of shape (n_samples, n_features)
            y_train: Binary labels of shape (n_samples,)
        """
        print(f"Training data shape: {X_train.shape}")
        print(f"Training labels shape: {y_train.shape}")

        # Prepare and store data info
        X_train_prep = self._prepare_data(X_train)
        self.n_features = X_train_prep.shape[1]

        print(f"Prepared training data shape: {X_train_prep.shape}")

        # Standardize the data
        X_train_scaled = self.scaler.fit_transform(X_train_prep)

        # Fit the logistic regression model
        self.model.fit(X_train_scaled, y_train)
        self.is_fitted = True

        # Print training accuracy
        train_pred = self.model.predict(X_train_scaled)
        train_acc = accuracy_score(y_train, train_pred)
        print(f"Training accuracy: {train_acc:.4f}")

    def predict(self, X_test: np.ndarray) -> np.ndarray:
        """
        Predict eye states for test data.

        Args:
            X_test: Test data of shape (n_samples, n_features)

        Returns:
            Binary predictions
        """
        if not self.is_fitted:
            raise ValueError("Model must be fitted before making predictions")

        print(f"Test data shape: {X_test.shape}")

        # Prepare test data
        X_test_prep = self._prepare_data(X_test)
        print(f"Prepared test data shape: {X_test_prep.shape}")

        # Scale using fitted scaler
        X_test_scaled = self.scaler.transform(X_test_prep)

        # Make predictions
        predictions = self.model.predict(X_test_scaled)
        return predictions

    def predict_proba(self, X_test: np.ndarray) -> np.ndarray:
        """
        Predict probabilities for test data.

        Args:
            X_test: Test data of shape (n_samples, n_features)

        Returns:
            Prediction probabilities
        """
        if not self.is_fitted:
            raise ValueError("Model must be fitted before making predictions")

        # Prepare test data
        X_test_prep = self._prepare_data(X_test)
        X_test_scaled = self.scaler.transform(X_test_prep)

        probabilities = self.model.predict_proba(X_test_scaled)
        return probabilities

    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray):
        """
        Evaluate the model on test data.

        Args:
            X_test: Test data
            y_test: True test labels
        """
        predictions = self.predict(X_test)
        probabilities = self.predict_proba(X_test)

        accuracy = accuracy_score(y_test, predictions)

        print(f"\nTest Evaluation:")
        print(f"Accuracy: {accuracy:.4f}")
        print(f"\nClassification Report:")
        print(classification_report(y_test, predictions))
        print(f"\nConfusion Matrix:")
        print(confusion_matrix(y_test, predictions))

        return {
            'accuracy': accuracy,
            'predictions': predictions,
            'probabilities': probabilities[:, 1]  # Probability of positive class
        }

# Example usage and demonstration
def generate_sample_data():
    """Generate sample data for demonstration."""
    np.random.seed(42)

    # Training data: 184 samples, 10 features each
    n_train_samples = 184
    n_features = 10

    X_train = np.random.randn(n_train_samples, n_features)

    # Generate binary labels (eye state: 0=closed, 1=open)
    # Make labels somewhat correlated with the data
    feature_weights = np.random.randn(n_features)
    scores = X_train @ feature_weights
    y_train = (scores > np.median(scores)).astype(int)

    # Test data: 40 samples, 10 features each
    n_test_samples = 40

    X_test = np.random.randn(n_test_samples, n_features)

    # Generate test labels using same weights
    test_scores = X_test @ feature_weights
    y_test = (test_scores > np.median(test_scores)).astype(int)

    return X_train, y_train, X_test, y_test

def main():
    """Main function to demonstrate the model."""
    print("=== Eye State Prediction with Logistic Regression ===\n")


    X_train = X_train_top
    X_test = X_test_top

    print(f"Training set: {X_train.shape} samples with labels {y_train.shape}")
    print(f"Test set: {X_test.shape} samples with labels {y_test.shape}")
    print(f"Training label distribution: {np.bincount(y_train)}")
    print(f"Test label distribution: {np.bincount(y_test)}")

    # Create and train model
    print("\n" + "="*50)
    print("Training the model...")
    model = TimeSeriesLogisticRegression()
    model.fit(X_train, y_train)

    # Make predictions and evaluate
    print("\n" + "="*50)
    print("Making predictions on test data...")
    results = model.evaluate(X_test, y_test)

    # Show some example predictions
    print("\n" + "="*50)
    print("Sample predictions:")
    predictions = results['predictions']
    probabilities = results['probabilities']

    for i in range(min(10, len(y_test))):
        print(f"Sample {i+1}: True={y_test[i]}, Pred={predictions[i]}, Prob={probabilities[i]:.3f}")

if __name__ == "__main__":
    main()
