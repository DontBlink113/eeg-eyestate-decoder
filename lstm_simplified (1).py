# -*- coding: utf-8 -*-
"""LSTM simplified

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-N7yy1BL3uHgRpdLcdhtVp_iQCinBVPW
"""

from google.colab import drive
from scipy.io import arff
import pandas as pd
import numpy as np
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Mount Google Drive
drive.mount('/content/drive', force_remount=True)

from google.colab import drive
from scipy.io import arff
import pandas as pd
import numpy as np
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

drive.mount('/content/drive', force_remount=True)
file_path = '/content/drive/MyDrive/Final_Project_Stats/EEG_Eye_State.arff'
data, meta = arff.loadarff(file_path)
df = pd.DataFrame(data)
data = df.to_numpy()
labels = data[:, 14]
decoded_labels = np.array([x.decode('utf-8') for x in labels]).astype(int)
data = np.delete(data, 14, axis=1).astype(np.float32)
left_hemisphere = [0, 3, 4, 5, 6, 12, 13]
right_hemisphere = [1, 2, 7, 8, 9, 10, 11]
X_left = data[:, left_hemisphere]
X_right = data[:, right_hemisphere]
lda = LinearDiscriminantAnalysis(n_components=1)
X_lda_left = lda.fit_transform(X_left, decoded_labels)
X_lda_right = lda.fit_transform(X_right, decoded_labels)
X_lda = np.concatenate((X_lda_left, X_lda_right), axis=1)
window_size = 128
step_size = 64
sequences = []
sequence_labels = []
for i in range(0, len(X_lda) - window_size + 1, step_size):
    seq = X_lda[i:i + window_size]
    label = decoded_labels[i + window_size - 1]
    sequences.append(seq)
    sequence_labels.append(label)
sequences = np.array(sequences)
sequence_labels = np.array(sequence_labels)

import numpy as np
from sklearn.model_selection import StratifiedKFold
from torch.utils.data import TensorDataset, DataLoader
import torch
import torch.nn as nn
from sklearn.metrics import accuracy_score

class LSTMEyeState(nn.Module):
    def __init__(self, input_size=2, hidden_size=32, num_layers=2, output_size=2):
        super(LSTMEyeState, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        out = self.softmax(out)
        return out

def run_10fold_cv(X, y, n_splits=10, batch_size=32, num_epochs=50, lr=0.001):
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=45)
    accuracies = []
    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]
        X_train_tensor = torch.FloatTensor(X_train).to(device)
        X_test_tensor = torch.FloatTensor(X_test).to(device)
        y_train_tensor = torch.LongTensor(y_train).to(device)
        y_test_tensor = torch.LongTensor(y_test).to(device)
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
        test_loader = DataLoader(test_dataset, batch_size=batch_size)
        model = LSTMEyeState().to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=lr)
        for epoch in range(num_epochs):
            model.train()
            for X_batch, y_batch in train_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                optimizer.zero_grad()
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                loss.backward()
                optimizer.step()
        model.eval()
        all_preds = []
        all_labels = []
        with torch.no_grad():
            for X_batch, y_batch in test_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                outputs = model(X_batch)
                _, predicted = torch.max(outputs.data, 1)
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(y_batch.cpu().numpy())
        fold_accuracy = accuracy_score(all_labels, all_preds)
        print(f'Fold {fold} - Accuracy: {fold_accuracy:.3f}')
        accuracies.append(fold_accuracy)
    mean_acc = np.mean(accuracies)
    std_acc = np.std(accuracies)
    print(f"\n=== 10-Fold Cross-Validation Results ===")
    print(f"Mean Accuracy: {mean_acc:.3f} Â± {std_acc:.3f} (over {n_splits} folds)")
    return mean_acc, std_acc

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
mean_acc, std_acc = run_10fold_cv(sequences, sequence_labels)